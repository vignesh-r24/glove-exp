{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from pprint import pprint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_embeddings(data_file):\n",
    "    word_emb = dict()\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            emb = np.array(values[1:], dtype='float32')\n",
    "            word_emb[word] = emb\n",
    "    return word_emb\n",
    "\n",
    "def closest_word(emb):\n",
    "    return sorted(glove_embeddings.keys(), key = lambda word: np.linalg.norm(glove_embeddings[word] - emb))\n",
    "\n",
    "def preprocess_string(test_str, tokenizer):\n",
    "    # convert the text sequence in train_x to integers\n",
    "    test_str_mod = tokenizer.texts_to_sequences(test_str)\n",
    "\n",
    "    # pad sequences so all are of the same length\n",
    "    test_str_mod = pad_sequences(test_str_mod, padding='post', maxlen=max_len)\n",
    "    return test_str_mod\n",
    "\n",
    "def define_model(embeddding_matrix, seq_len, vocab_size):\n",
    "    model = Sequential()\n",
    "    embeddding_layer = Embedding(input_dim=vocab_size,\n",
    "                                 output_dim=50,\n",
    "                                 weights = [embedding_matrix], \n",
    "                                 input_length=max_len,\n",
    "                                 trainable=False)\n",
    "    # converts input to the shape (max_len, 50)\n",
    "    model.add(embeddding_layer)\n",
    "    # converts the 2D output to (max*50) units\n",
    "    model.add(Flatten())\n",
    "    # finally, all units' measure will be used to classify +ve / -ve\n",
    "    # with sigmoid as activation function\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = './data/glove.6B/glove.6B.50d.txt'\n",
    "glove_embeddings = read_word_embeddings(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dummy dataset for sentiment analysis\n",
    "# this dataset will contain reviews about a grocery store\n",
    "train_x = [\"This is the final destination for fresh produce\",\n",
    "          \"The cashier was extremely friendly\",\n",
    "          \"Most of the seafood was stale and infested with rodents\",\n",
    "          \"Poor customer service\",\n",
    "          \"Long waiting time and rude staff members\",\n",
    "          \"Could not be more happy with my purchase\",\n",
    "          \"Unhealthy in the long term\",\n",
    "          \"Easily accesible from any corner of the city\",\n",
    "          \"Well organized store\",\n",
    "          \"Terrible shopping experience\"]\n",
    "# 0 = positive , 1 = negative\n",
    "train_y = [0,0,1,1,1,0,1,0,0,1]\n",
    "# maximum sequence length\n",
    "max_len = max([len(sentence.split(' ')) for sentence in train_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert words to indices and rank them on frequency\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# examine frequently occuring words\n",
    "# pprint(tokenizer.index_word)\n",
    "\n",
    "# vocab size = total no. of unique word + 1, 0 is reserved index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# convert the text sequence in train_x to integers\n",
    "train_x_mod = tokenizer.texts_to_sequences(train_x)\n",
    "# pprint(train_x_mod[0])\n",
    "\n",
    "# pad sequences so all are of the same length\n",
    "train_x_mod = pad_sequences(train_x_mod, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding matrix using pretrained glove vector\n",
    "# we are using a 50-D vector for each word\n",
    "# therefore output matrix dim would be (vocab_size X 50)\n",
    "embedding_matrix = np.zeros((vocab_size, 50), dtype='float32')\n",
    "# for each word in vocab, replace its index in train_x by correponding GloVe vector\n",
    "for (index, word) in tokenizer.index_word.items():\n",
    "    glove_vector = glove_embeddings.get(word)\n",
    "    if glove_vector is not None:\n",
    "        embedding_matrix[index] = glove_vector\n",
    "    # Note: It is unlikely that the vocab words will not exist in glove dict\n",
    "    # If they don't they will be represented by the zero vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 10, 50)            2600      \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 3,101\n",
      "Trainable params: 501\n",
      "Non-trainable params: 2,600\n",
      "_________________________________________________________________\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Model Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate the model\n",
    "model = define_model(embedding_matrix, max_len, vocab_size)\n",
    "model.fit(train_x_mod, train_y, epochs=20, verbose=0)\n",
    "loss, accuracy = model.evaluate(train_x_mod, train_y)\n",
    "print(f'Model Accuracy: {(accuracy*100.0):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83391154]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the sentiment of a new string: 0=postitive, 1=negative\n",
    "# Note that if we use an out-of-vocab word in our test string \\\n",
    "# the tokenizer will simply encode them as 0\n",
    "# Also if len(test_str) > max_len, it will be truncated to max_len\n",
    "test_str = [\"infested with rodents\"]\n",
    "test_str_mod = preprocess_string(test_str, tokenizer)\n",
    "model.predict(test_str_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[0.61833775]]\n",
      "Test string encoding: [[23 24  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# since our vocab size is limited, we aren't making the most\n",
    "# out of our context-aware GloVe word embeddings\n",
    "# For example, if we were to use the word 'Pathetic',\n",
    "# its GloVe representation would be close to that of 'Poor'\n",
    "# But 'Pathetic' is out-of-vocab, so it will be ignored during tokenization\n",
    "test_str = [\"Pathetic customer service\"]\n",
    "test_str_mod = preprocess_string(test_str, tokenizer)\n",
    "print(f'Prediction: {model.predict(test_str_mod)}')\n",
    "print(f'Test string encoding: {test_str_mod}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[0.63508445]]\n",
      "Test string encoding: [[22 50 51  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# let's try with a sample from our training dataset\n",
    "test_str = [\"Poor shopping experience\"]\n",
    "test_str_mod = preprocess_string(test_str, tokenizer)\n",
    "print(f'Prediction: {model.predict(test_str_mod)}')\n",
    "print(f'Test string encoding: {test_str_mod}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
